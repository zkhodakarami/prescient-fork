{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaH5Y0UGVlO0+I60Wg9qX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zkhodakarami/prescient-fork/blob/master/Trials_pre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OmXPHyhOna_",
        "outputId": "fce8ff09-3a7e-40b9-8807-329f8c3d0a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "expr2 = pd.read_csv(\"/content/drive/MyDrive/NF/DGB_20181016_COLO858_updated_20200519_filtered.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "ms_sXmfvO4ew"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_num = expr2\n",
        "del exp_num[\"Timepoint\"]\n",
        "del exp_num[\"Drugs\"]\n",
        "del exp_num[\"Doses\"]\n",
        "exp_num.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "AW8mDpLIPB5R",
        "outputId": "f95d66f6-28e6-43f9-88c7-29b25f2faae9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Phospho_c_Jun  Phospho_ATF2  Phospho_Fra1     c_Fos     c_Jun  \\\n",
              "Phospho_c_Fos                                                                  \n",
              "2.482309            2.566314      2.347076      3.725452  2.717293  2.757141   \n",
              "2.507659            2.443444      2.363911      3.809165  2.723162  2.509363   \n",
              "3.128445            3.230032      2.690699      4.355289  3.304874  3.651961   \n",
              "2.614351            2.583048      2.399141      3.928000  2.740837  2.625010   \n",
              "2.269206            2.434744      2.345996      3.746324  2.542989  2.804418   \n",
              "\n",
              "                   Fra1      JunD      ATF2      JunB      Fra2  ...  \\\n",
              "Phospho_c_Fos                                                    ...   \n",
              "2.482309       3.817022  3.534199  3.111005  3.634476  2.981213  ...   \n",
              "2.507659       3.793575  3.320416  2.923885  3.523222  2.656339  ...   \n",
              "3.128445       4.184460  3.669419  3.140430  3.880478  3.666164  ...   \n",
              "2.614351       3.883700  3.346847  2.925226  3.679446  2.920995  ...   \n",
              "2.269206       3.697967  3.149210  2.916250  3.338552  2.935117  ...   \n",
              "\n",
              "               Phospho_p38     Ki_67  NF_kappaB     MiTFg       AXL     Sox10  \\\n",
              "Phospho_c_Fos                                                                   \n",
              "2.482309          3.041194  3.751343   3.547431  3.736449  3.373901  3.737063   \n",
              "2.507659          2.961434  3.861726   3.508702  3.630648  3.643372  3.597944   \n",
              "3.128445          3.063199  4.035174   3.591757  3.549209  4.065621  3.870810   \n",
              "2.614351          3.006989  4.182933   3.760017  3.799474  3.742439  3.683257   \n",
              "2.269206          2.796205  3.980446   3.591768  3.422480  3.822521  3.533335   \n",
              "\n",
              "               timepoint_id  drug_id  dose_id  Rep  \n",
              "Phospho_c_Fos                                       \n",
              "2.482309                  1        1        1    1  \n",
              "2.507659                  1        1        1    1  \n",
              "3.128445                  1        1        1    1  \n",
              "2.614351                  1        1        1    1  \n",
              "2.269206                  1        1        1    1  \n",
              "\n",
              "[5 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d64d7bac-556c-4f93-a01b-310102070eb8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phospho_c_Jun</th>\n",
              "      <th>Phospho_ATF2</th>\n",
              "      <th>Phospho_Fra1</th>\n",
              "      <th>c_Fos</th>\n",
              "      <th>c_Jun</th>\n",
              "      <th>Fra1</th>\n",
              "      <th>JunD</th>\n",
              "      <th>ATF2</th>\n",
              "      <th>JunB</th>\n",
              "      <th>Fra2</th>\n",
              "      <th>...</th>\n",
              "      <th>Phospho_p38</th>\n",
              "      <th>Ki_67</th>\n",
              "      <th>NF_kappaB</th>\n",
              "      <th>MiTFg</th>\n",
              "      <th>AXL</th>\n",
              "      <th>Sox10</th>\n",
              "      <th>timepoint_id</th>\n",
              "      <th>drug_id</th>\n",
              "      <th>dose_id</th>\n",
              "      <th>Rep</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Phospho_c_Fos</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2.482309</th>\n",
              "      <td>2.566314</td>\n",
              "      <td>2.347076</td>\n",
              "      <td>3.725452</td>\n",
              "      <td>2.717293</td>\n",
              "      <td>2.757141</td>\n",
              "      <td>3.817022</td>\n",
              "      <td>3.534199</td>\n",
              "      <td>3.111005</td>\n",
              "      <td>3.634476</td>\n",
              "      <td>2.981213</td>\n",
              "      <td>...</td>\n",
              "      <td>3.041194</td>\n",
              "      <td>3.751343</td>\n",
              "      <td>3.547431</td>\n",
              "      <td>3.736449</td>\n",
              "      <td>3.373901</td>\n",
              "      <td>3.737063</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.507659</th>\n",
              "      <td>2.443444</td>\n",
              "      <td>2.363911</td>\n",
              "      <td>3.809165</td>\n",
              "      <td>2.723162</td>\n",
              "      <td>2.509363</td>\n",
              "      <td>3.793575</td>\n",
              "      <td>3.320416</td>\n",
              "      <td>2.923885</td>\n",
              "      <td>3.523222</td>\n",
              "      <td>2.656339</td>\n",
              "      <td>...</td>\n",
              "      <td>2.961434</td>\n",
              "      <td>3.861726</td>\n",
              "      <td>3.508702</td>\n",
              "      <td>3.630648</td>\n",
              "      <td>3.643372</td>\n",
              "      <td>3.597944</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.128445</th>\n",
              "      <td>3.230032</td>\n",
              "      <td>2.690699</td>\n",
              "      <td>4.355289</td>\n",
              "      <td>3.304874</td>\n",
              "      <td>3.651961</td>\n",
              "      <td>4.184460</td>\n",
              "      <td>3.669419</td>\n",
              "      <td>3.140430</td>\n",
              "      <td>3.880478</td>\n",
              "      <td>3.666164</td>\n",
              "      <td>...</td>\n",
              "      <td>3.063199</td>\n",
              "      <td>4.035174</td>\n",
              "      <td>3.591757</td>\n",
              "      <td>3.549209</td>\n",
              "      <td>4.065621</td>\n",
              "      <td>3.870810</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.614351</th>\n",
              "      <td>2.583048</td>\n",
              "      <td>2.399141</td>\n",
              "      <td>3.928000</td>\n",
              "      <td>2.740837</td>\n",
              "      <td>2.625010</td>\n",
              "      <td>3.883700</td>\n",
              "      <td>3.346847</td>\n",
              "      <td>2.925226</td>\n",
              "      <td>3.679446</td>\n",
              "      <td>2.920995</td>\n",
              "      <td>...</td>\n",
              "      <td>3.006989</td>\n",
              "      <td>4.182933</td>\n",
              "      <td>3.760017</td>\n",
              "      <td>3.799474</td>\n",
              "      <td>3.742439</td>\n",
              "      <td>3.683257</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.269206</th>\n",
              "      <td>2.434744</td>\n",
              "      <td>2.345996</td>\n",
              "      <td>3.746324</td>\n",
              "      <td>2.542989</td>\n",
              "      <td>2.804418</td>\n",
              "      <td>3.697967</td>\n",
              "      <td>3.149210</td>\n",
              "      <td>2.916250</td>\n",
              "      <td>3.338552</td>\n",
              "      <td>2.935117</td>\n",
              "      <td>...</td>\n",
              "      <td>2.796205</td>\n",
              "      <td>3.980446</td>\n",
              "      <td>3.591768</td>\n",
              "      <td>3.422480</td>\n",
              "      <td>3.822521</td>\n",
              "      <td>3.533335</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d64d7bac-556c-4f93-a01b-310102070eb8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d64d7bac-556c-4f93-a01b-310102070eb8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d64d7bac-556c-4f93-a01b-310102070eb8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp_num.to_csv('file.csv')"
      ],
      "metadata": {
        "id": "HMY-ZmIdPE16"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install umap\n",
        "#!pip install git+https://github.com/hyperopt/hyperopt-sklearn.git\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import pyreadr\n",
        "#import scanpy as sc\n",
        "#import anndata\n",
        "import sklearn\n",
        "#import umap\n",
        "#import annoy\n",
        "import torch\n",
        "\n"
      ],
      "metadata": {
        "id": "eYZ-7dJ9P3Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip show scikit-learn\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmpCoilKSR0x",
        "outputId": "f54ebd92-641b-47df-f4df-145ffb71859f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Name: scikit-learn\n",
            "Version: 1.0.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: threadpoolctl, numpy, scipy, joblib\n",
            "Required-by: yellowbrick, sklearn-pandas, qudida, mlxtend, lightgbm, librosa, imbalanced-learn, fastai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall umap\n",
        "!pip install umap-learn\n",
        "import umap.umap_ as umap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZcpJo_tT6jj",
        "outputId": "69c38a1f-6ea7-4733-b1f0-53a275da0c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping umap as it is not installed.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from umap-learn) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (5.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.11.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=ab728947c313b40ab44454a2578d336bc0dafa12bc89f8edc8076ae2c12c0e51\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/3a/67/06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=6736f3c21a3fd16d9a53d28fdc09244c8bacdd45cafe3f2eb3a39038d64a16fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/63/3a/29954bca1a27ba100ed8c27973a78cb71b43dc67aed62e80c3\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.8 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is my function (based on this) to clean the dataset of nan, Inf, and missing cells (for skewed datasets):\n",
        "def clean_dataset(df):\n",
        "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
        "    df.dropna(inplace=True)\n",
        "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
        "    return df[indices_to_keep].astype(np.float64)"
      ],
      "metadata": {
        "id": "KIo9orAhVCww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn import decomposition\n",
        "\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "pca = sklearn.decomposition.PCA(15)\n",
        "um = umap.UMAP(n_components = 2, metric = 'euclidean', n_neighbors = 15)\n",
        "expr = pd.read_csv('file.csv', index_col=0)\n",
        "clean_dataset(expr)\n",
        "\n",
        "expr = expr.reset_index()\n",
        "x = scaler.fit_transform(expr)\n",
        "\n",
        "xp = pca.fit_transform(x)\n",
        "xu = um.fit_transform(xp)"
      ],
      "metadata": {
        "id": "0HSvq5T8Rjf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0].shape[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwRAaS9dgTPI",
        "outputId": "5aaba696-fbd5-4272-df96-db226a328380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write as a torch object\n",
        "torch.save({\n",
        "  \"data\": expr,\n",
        "  \"x\":x,\n",
        "  \"xp\":xp,\n",
        "  \"xu\": xu,\n",
        "  }, \"data.pt\")"
      ],
      "metadata": {
        "id": "r5VxOWSHWy3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-el3xBNLgRAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geomloss\n",
        "!python /content/drive/MyDrive/priscient/prescient/train/model.py\n",
        "!python /content/drive/MyDrive/priscient/prescient/train/run.py\n",
        "!python /content/drive/MyDrive/priscient/prescient/train/__init__.py\n",
        "!python /content/drive/MyDrive/priscient/prescient/train/util.py\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/priscient/prescient')\n",
        "!pip install train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3kVI7xOeQz5",
        "outputId": "22fdba08-e7bb-4e80-8503-aeb5f077dcda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting geomloss\n",
            "  Downloading geomloss-0.2.5.tar.gz (26 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from geomloss) (1.21.6)\n",
            "Building wheels for collected packages: geomloss\n",
            "  Building wheel for geomloss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for geomloss: filename=geomloss-0.2.5-py3-none-any.whl size=32069 sha256=827a98cde367490a08299ac960eae6384f5f49291eefc07c0f1f965503c99d6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/ba/ac/e61f794175073650207d2ad91bea30e2553fc83f74fc9a6b82\n",
            "Successfully built geomloss\n",
            "Installing collected packages: geomloss\n",
            "Successfully installed geomloss-0.2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting train\n",
            "  Downloading train-0.0.5.tar.gz (8.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from train) (1.21.6)\n",
            "Building wheels for collected packages: train\n",
            "  Building wheel for train (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for train: filename=train-0.0.5-py3-none-any.whl size=8506 sha256=b854cdaf6ff9011a1bf5682316db3e42f1546fb18a498cd8aa1db4f2dcf62b45\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/20/c5/a817b3416832aadd9b2f0cb859ef4efb1afadf3cf0fd619f00\n",
            "Successfully built train\n",
            "Installing collected packages: train\n",
            "Successfully installed train-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import itertools\n",
        "import json\n",
        "import sklearn.decomposition"
      ],
      "metadata": {
        "id": "CeJL-N40eRTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geomloss import SamplesLoss\n",
        "from collections import OrderedDict\n",
        "from types import SimpleNamespace\n",
        "from time import strftime, localtime\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/priscient/prescient/train')\n",
        "import model\n",
        "import run\n",
        "import util\n",
        "import train\n",
        "import train"
      ],
      "metadata": {
        "id": "wIW3YjlEfsaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_config(args):\n",
        "\n",
        "    config = SimpleNamespace(\n",
        "\n",
        "        seed = args.seed,\n",
        "        timestamp = strftime(\"%a, %d %b %Y %H:%M:%S\", localtime()),\n",
        "\n",
        "        # data parameters\n",
        "        data_path = args.data_path,\n",
        "#        weight = args.weight,\n",
        "\n",
        "        # model parameters\n",
        "        activation = args.activation,\n",
        "        layers = args.layers,\n",
        "        k_dim = args.k_dim,\n",
        "\n",
        "        # pretraining parameters\n",
        "        pretrain_burnin = 50,\n",
        "        pretrain_sd = 0.1,\n",
        "        pretrain_lr = 1e-9,\n",
        "        pretrain_epochs = 10,\n",
        "\n",
        "        # training parameters\n",
        "        train_dt = args.train_dt,\n",
        "        train_sd = args.train_sd,\n",
        "        train_batch_size = 10,\n",
        "        ns = 2000,\n",
        "        train_burnin = 100,\n",
        "        train_tau = args.train_tau,\n",
        "        train_epochs = 10,\n",
        "        train_lr = args.train_lr,\n",
        "        train_clip = args.train_clip,\n",
        "        save = args.save,\n",
        "\n",
        "        # loss parameters\n",
        "        sinkhorn_scaling = 0.7,\n",
        "        sinkhorn_blur = 0.1,\n",
        "\n",
        "        # file parameters\n",
        "        out_dir = args.out_dir,\n",
        "        out_name = args.out_dir.split('/')[-1],\n",
        "        pretrain_pt = os.path.join(args.out_dir, 'pretrain.pt'),\n",
        "        train_pt = os.path.join(args.out_dir, 'train.{}.pt'),\n",
        "        train_log = os.path.join(args.out_dir, 'train.log'),\n",
        "        done_log = os.path.join(args.out_dir, 'done.log'),\n",
        "        config_pt = os.path.join(args.out_dir, 'config.pt'),\n",
        "    )\n",
        "\n",
        "    config.train_t = []\n",
        "    config.test_t = []\n",
        "\n",
        "    if not os.path.exists(args.out_dir):\n",
        "        print('Making directory at {}'.format(args.out_dir))\n",
        "        os.makedirs(args.out_dir)\n",
        "    else:\n",
        "        print('Directory exists at {}'.format(args.out_dir))\n",
        "    return config\n",
        "\n"
      ],
      "metadata": {
        "id": "QIxacAp4ftrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_parser():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action = 'store_true')\n",
        "    parser.add_argument('--gpu', default = 7, type = int, help=\"Designate GPU number as an integer (compatible with CUDA).\")\n",
        "    parser.add_argument('--out_dir', default = './experiments', help=\"Directory for storing training output.\")\n",
        "    parser.add_argument('--seed', type = int, default = 2, help=\"Set seed for training process.\")\n",
        "    # -- data options\n",
        "    parser.add_argument('-i', '--data_path', required=True, help=\"Input PRESCIENT data torch file.\")\n",
        "    parser.add_argument('--weight_name', default = None, help=\"Designate descriptive name of growth parameters for filename.\")\n",
        "    # -- model options\n",
        "    parser.add_argument('--loss', default = 'euclidean', help=\"Designate distance function for loss.\")\n",
        "    parser.add_argument('--k_dim', default = 500, type = int, help=\"Designate hidden units of NN.\")\n",
        "    parser.add_argument('--activation', default = 'softplus', help=\"Designate activation function for layers of NN.\")\n",
        "    parser.add_argument('--layers', default = 1, type = int, help=\"Choose number of layers for neural network parameterizing the potential function.\")\n",
        "    # -- pretrain options\n",
        "    parser.add_argument('--pretrain_epochs', default = 500, type = int, help=\"Number of epochs for pretraining with contrastive divergence.\")\n",
        "    # -- train options\n",
        "    parser.add_argument('--train_epochs', default = 2500, type = int, help=\"Number of epochs for training.\")\n",
        "    parser.add_argument('--train_lr', default = 0.01, type = float, help=\"Learning rate for Adam optimizer during training.\")\n",
        "    parser.add_argument('--train_dt', default = 0.1, type = float, help=\"Timestep for simulations during training.\")\n",
        "    parser.add_argument('--train_sd', default = 0.5, type = float, help=\"Standard deviation of Gaussian noise for simulation steps.\")\n",
        "    parser.add_argument('--train_tau', default = 1e-6, type = float, help=\"Tau hyperparameter of PRESCIENT.\")\n",
        "    parser.add_argument('--train_batch', default = 0.1, type = float, help=\"Batch size for training.\")\n",
        "    parser.add_argument('--train_clip', default = 0.25, type = float, help=\"Gradient clipping threshold for training.\")\n",
        "    parser.add_argument('--save', default = 100, type = int, help=\"Save model every n epochs.\")\n",
        "    # -- run options\n",
        "    parser.add_argument('--pretrain', type=bool, default=True, help=\"If True, pretraining will run.\")\n",
        "    parser.add_argument('--train', type=bool, default=True, help=\"If True, training will run with existing pretraining torch file.\")\n",
        "    parser.add_argument('--config')\n",
        "    return parser\n",
        "\n",
        "def train_init(args):\n",
        "\n",
        "    a = copy.copy(args)\n",
        "\n",
        "    # data\n",
        "    data_pt = \"data.pt\"\n",
        "    x = data_pt\n",
        "#    y = data_pt[\"y\"]\n",
        "#    weight = data_pt[\"w\"]\n",
        "#    if args.weight_name != None:\n",
        "#        a.weight = args.weight_name\n",
        "    # weight = os.path.basename(a.weight_path)\n",
        "    # weight = weight.split('.')[0].split('-')[-1]\n",
        "\n",
        "\n",
        "    # out directory\n",
        "    name = (\n",
        "#        \"{weight}-\"\n",
        "        \"{activation}_{layers}_{k_dim}-\"\n",
        "        \"{train_tau}\"\n",
        "    ).format(**a.__dict__)\n",
        "\n",
        "    out_dir = os.path.join('experiments', 'seed_{}'.format(a.seed))\n",
        "    config = init_config(a)\n",
        "\n",
        "    config.x_dim = x[0].shape[-1]\n",
        "#    config.t = y[-1] - y[0]\n",
        "\n",
        "#    config.start_t = y[0]\n",
        "#    config.train_t = y[1:]\n",
        "#    y_start = y[config.start_t]\n",
        "#    y_ = [y_ for y_ in y if y_ > y_start]\n",
        "\n",
        "#    w_ = weight[config.start_t]\n",
        "#    w = {(y_start, yy): torch.from_numpy(np.exp((yy - y_start)*w_)) for yy in y_}\n",
        "\n",
        "    return x, config\n"
      ],
      "metadata": {
        "id": "4tqwVbuArhqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#util\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "from types import SimpleNamespace\n",
        "from time import strftime, localtime\n",
        "\n",
        "import argparse\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import sklearn.decomposition\n",
        "\n",
        "# ---- convenience functions\n",
        "\n",
        "def p_samp(p, num_samp, w = None):\n",
        "    repflag = p.shape[0] < num_samp\n",
        "    p_sub = np.random.choice(p.shape[0], size = num_samp, replace = repflag)\n",
        "    if w is None:\n",
        "        w_ = torch.ones(len(p_sub))\n",
        "    else:\n",
        "        w_ = w[p_sub].clone()\n",
        "    w_ = w_ / w_.sum()\n",
        "\n",
        "    return p[p_sub,:].clone(), w_\n",
        "\n",
        "def fit_regularizer(samples, pp, burnin, dt, sd, model, device):\n",
        "\n",
        "    factor = samples.shape[0] / pp.shape[0]\n",
        "\n",
        "    z = torch.randn(burnin, pp.shape[0], pp.shape[1]) * sd\n",
        "    z = z.to(device)\n",
        "\n",
        "    for i in range(burnin):\n",
        "        pp = model._step(pp, dt, z = z[i,:,:])\n",
        "\n",
        "    pos_fv = -1 * model._pot(samples).sum()\n",
        "    neg_fv = factor * model._pot(pp.detach()).sum()\n",
        "\n",
        "    return pp, pos_fv, neg_fv\n",
        "\n",
        "def pca_transform(x):\n",
        "\n",
        "    pca = sklearn.decomposition.PCA(n_components = 50)\n",
        "    # keep track of how to break up the array after concat\n",
        "    x_ = torch.cat(x)\n",
        "    x_ = pca.fit_transform(x_)\n",
        "    x_breaks = np.append([0], np.cumsum([len(x_) for x_ in x]))\n",
        "    x_tmp = []\n",
        "    for i in range(len(x_breaks) - 1):\n",
        "        ii = x_breaks[i]\n",
        "        jj = x_breaks[i+1]\n",
        "        x_tmp.append(torch.from_numpy(x_[ii:jj]).float())\n",
        "    x = x_tmp\n",
        "\n",
        "    return x\n",
        "\n",
        "def get_weight(w, time_elapsed):\n",
        "    return w\n",
        "\n",
        "#def init(args):\n",
        "\n",
        "#    args.cuda = torch.cuda.is_available()\n",
        "#    device = torch.device('cuda:{}')\n",
        "#    kwargs = {'num_workers': 1, 'pin_memory': True} \n",
        "\n",
        " #   return device, kwargs\n",
        "\n",
        "def weighted_samp(p, num_samp, w):\n",
        "    ix = list(torch.utils.data.WeightedRandomSampler(w, num_samp))\n",
        "    return p[ix,:].clone()"
      ],
      "metadata": {
        "id": "_sQOybuLg6hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "import tqdm\n",
        "\n",
        "from collections import OrderedDict\n",
        "from types import SimpleNamespace\n",
        "from time import strftime, localtime\n",
        "\n",
        "import argparse\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import sklearn.decomposition\n",
        "\n",
        "#from .util import *\n",
        "\n",
        "# ---- PRESCIENT\n",
        "\n",
        "class IntReLU(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(IntReLU, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.max(torch.zeros_like(x), 0.5 * (x**2)) # + self.c)\n",
        "\n",
        "\n",
        "class AutoGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(AutoGenerator, self).__init__()\n",
        "\n",
        "        self.x_dim = config.x_dim\n",
        "        self.k_dim = config.k_dim\n",
        "        self.layers = config.layers\n",
        "\n",
        "        self.activation = config.activation\n",
        "        if self.activation == 'relu':\n",
        "            self.act = nn.LeakyReLU\n",
        "        elif self.activation == 'softplus':\n",
        "            self.act = nn.Softplus\n",
        "        elif self.activation == 'intrelu': # broken, wip\n",
        "            raise NotImplementedError\n",
        "        elif self.activation == 'none':\n",
        "            self.act = None\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.net_ = []\n",
        "        for i in range(self.layers):\n",
        "            # add linear layer\n",
        "            if i == 0:\n",
        "                self.net_.append(('linear{}'.format(i+1), nn.Linear(self.x_dim, self.k_dim)))\n",
        "            else:\n",
        "                self.net_.append(('linear{}'.format(i+1), nn.Linear(self.k_dim, self.k_dim)))\n",
        "            # add activation\n",
        "            if self.activation == 'intrelu':\n",
        "                raise NotImplementedError\n",
        "            elif self.activation == 'none':\n",
        "                pass\n",
        "            else:\n",
        "                self.net_.append(('{}{}'.format(self.activation, i+1), self.act()))\n",
        "        self.net_.append(('linear', nn.Linear(self.k_dim, 1, bias = False)))\n",
        "        self.net_ = OrderedDict(self.net_)\n",
        "        self.net = nn.Sequential(self.net_)\n",
        "\n",
        "        net_params = list(self.net.parameters())\n",
        "        net_params[-1].data = torch.zeros(net_params[-1].data.shape) # initialize\n",
        "\n",
        "    def _step(self, x, dt, z):\n",
        "        sqrtdt = np.sqrt(dt)\n",
        "        return x + self._drift(x) * dt + z * sqrtdt\n",
        "\n",
        "    def _pot(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def _drift(self, x):\n",
        "        x_ = x.requires_grad_()\n",
        "        pot = self._pot(x_)\n",
        "\n",
        "        drift = torch.autograd.grad(pot, x_, torch.ones_like(pot),\n",
        "            create_graph = True)[0]\n",
        "        return drift\n",
        "\n",
        "# ---- loss\n",
        "\n",
        "class OTLoss():\n",
        "\n",
        "    def __init__(self, config, device):\n",
        "\n",
        "        self.ot_solver = SamplesLoss(\"sinkhorn\", p = 2, blur = config.sinkhorn_blur,\n",
        "            scaling = config.sinkhorn_scaling, debias = True)\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, a_i, x_i, b_j, y_j, requires_grad = True):\n",
        "\n",
        "        a_i = a_i.to(self.device)\n",
        "        x_i = x_i.to(self.device)\n",
        "        b_j = b_j.to(self.device)\n",
        "        y_j = y_j.to(self.device)\n",
        "\n",
        "        if requires_grad:\n",
        "            a_i.requires_grad_()\n",
        "            x_i.requires_grad_()\n",
        "            b_j.requires_grad_()\n",
        "\n",
        "        loss_xy = self.ot_solver(a_i, x_i, b_j, y_j)\n",
        "        return loss_xy"
      ],
      "metadata": {
        "id": "Zy5u0pIatNSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tqdm\n",
        "from time import strftime, localtime\n",
        "\n",
        "#from .model import *\n",
        "#from .util import *\n",
        "\n",
        "def run(args, init_task):\n",
        "\n",
        "    # ---- initialize\n",
        "\n",
        "#    device, kwargs = init(args)\n",
        "    torch.manual_seed(30)\n",
        "    np.random.seed(30)\n",
        "\n",
        "    x, y, w, config = init_task(args)\n",
        "\n",
        "    # ---- model\n",
        "\n",
        "    model = AutoGenerator(config)\n",
        "    print(model)\n",
        "    model.zero_grad()\n",
        "\n",
        "    # ---- loss\n",
        "\n",
        "    if args.loss == 'euclidean':\n",
        "        loss = OTLoss(config, device)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    torch.save(config.__dict__, config.config_pt)\n",
        "\n",
        "    if args.pretrain:\n",
        "\n",
        "        if os.path.exists(config.done_log):\n",
        "\n",
        "            print(config.done_log, ' exists. Skipping.')\n",
        "\n",
        "        else:\n",
        "\n",
        "            model.to(device)\n",
        "            x_last = x[config.train_t[-1]].to(device) # use the last available training point\n",
        "            optimizer = optim.SGD(list(model.parameters()), lr = config.pretrain_lr)\n",
        "\n",
        "            pbar = tqdm.tqdm(range(config.pretrain_epochs))\n",
        "            for epoch in pbar:\n",
        "\n",
        "                pp, _ = p_samp(x_last, config.ns)\n",
        "\n",
        "                dt = config.t / config.pretrain_burnin\n",
        "                pp, pos_fv, neg_fv = fit_regularizer(x_last, pp,\n",
        "                    config.pretrain_burnin, dt, config.pretrain_sd,\n",
        "                    model, device)\n",
        "                fv_tot = pos_fv + neg_fv\n",
        "\n",
        "                fv_tot.backward()\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "\n",
        "                pbar.set_description('[{}|pretrain] {} {:.3f}'.format(\n",
        "                    config.out_name, epoch, fv_tot.item()))\n",
        "\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "            }, config.pretrain_pt)\n",
        "\n",
        "    if args.train:\n",
        "\n",
        "        if os.path.exists(config.done_log):\n",
        "\n",
        "            print(config.done_log, ' exists. Skipping.')\n",
        "\n",
        "        else:\n",
        "\n",
        "            checkpoint = torch.load(config.pretrain_pt)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model.to(device)\n",
        "\n",
        "            optimizer = optim.Adam(list(model.parameters()), lr = config.train_lr)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.9)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pbar = tqdm.tqdm(range(config.train_epochs))\n",
        "            x_last = x[config.train_t[-1]].to(device) # use the last available training point\n",
        "            # fit on time points\n",
        "\n",
        "            best_train_loss_xy = np.inf\n",
        "            log_handle = open(config.train_log, 'w')\n",
        "\n",
        "            for epoch in pbar:\n",
        "\n",
        "                losses_xy = []\n",
        "                config.train_epoch = epoch\n",
        "\n",
        "                for j in config.train_t:\n",
        "\n",
        "                    t_cur = j\n",
        "                    t_prev = config.start_t\n",
        "                    dat_cur = x[t_cur]\n",
        "                    dat_prev = x[t_prev]\n",
        "                    y_cur = y[t_cur]\n",
        "                    y_prev = y[t_prev]\n",
        "                    time_elapsed = y_cur - y_prev\n",
        "\n",
        "                    w_prev = get_weight(w[(y_prev, y_cur)], time_elapsed)\n",
        "\n",
        "                    x_i, a_i = p_samp(dat_prev, int(dat_prev.shape[0] * args.train_batch),\n",
        "                        w_prev)\n",
        "                    x_i = x_i.to(device)\n",
        "                    num_steps = int(np.round(time_elapsed / config.train_dt))\n",
        "                    for _ in range(num_steps):\n",
        "                        z = torch.randn(x_i.shape[0], x_i.shape[1]) * config.train_sd\n",
        "                        z = z.to(device)\n",
        "                        x_i = model._step(x_i, dt = config.train_dt, z = z)\n",
        "\n",
        "                    y_j, b_j = p_samp(dat_cur, int(dat_cur.shape[0] * args.train_batch))\n",
        "\n",
        "                    loss_xy = loss(a_i, x_i, b_j, y_j)\n",
        "                    losses_xy.append(loss_xy.item())\n",
        "\n",
        "                    #[F_i, G_j, dx_i] = torch.autograd.grad( Loss_xy, [a_i, b_j, x_i] )\n",
        "\n",
        "                    loss_xy.backward()\n",
        "\n",
        "                train_loss_xy = np.mean(losses_xy)\n",
        "\n",
        "                # fit regularizer\n",
        "\n",
        "                if config.train_tau > 0:\n",
        "\n",
        "                    pp, _ = p_samp(x_last, config.ns)\n",
        "\n",
        "                    dt = config.t / config.train_burnin\n",
        "                    pp, pos_fv, neg_fv = fit_regularizer(x_last, pp,\n",
        "                        config.train_burnin, dt, config.train_sd,\n",
        "                        model, device)\n",
        "                    fv_tot = pos_fv + neg_fv\n",
        "                    fv_tot *= config.train_tau\n",
        "                    fv_tot.backward()\n",
        "\n",
        "                # step\n",
        "\n",
        "                if config.train_clip > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.train_clip)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "\n",
        "                # report\n",
        "\n",
        "                desc = \"[{}|train] {}\".format(config.out_name, epoch + 1)\n",
        "                if len(losses_xy) < 10:\n",
        "                    for l_xy in losses_xy:\n",
        "                        desc += \" {:.6f}\".format(l_xy)\n",
        "                desc += \" {:.6f}\".format(train_loss_xy)\n",
        "                desc += \" {:.6f}\".format(best_train_loss_xy)\n",
        "                pbar.set_description(desc)\n",
        "                log_handle.write(desc + '\\n')\n",
        "                log_handle.flush()\n",
        "\n",
        "                if train_loss_xy < best_train_loss_xy:\n",
        "                    best_train_loss_xy = train_loss_xy\n",
        "\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'epoch': config.train_epoch + 1,\n",
        "                    }, config.train_pt.format('best'))\n",
        "\n",
        "                # save model every x epochs\n",
        "\n",
        "                if (config.train_epoch + 1) % config.save == 0:\n",
        "                    epoch_ = str(config.train_epoch + 1).rjust(6, '0')\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'epoch': config.train_epoch + 1,\n",
        "                    }, config.train_pt.format('epoch_{}'.format(epoch_)))\n",
        "\n",
        "            log_handle.close()\n",
        "\n",
        "            log_handle = open(config.done_log, 'w')\n",
        "            timestamp = strftime(\"%a, %d %b %Y %H:%M:%S\", localtime())\n",
        "            log_handle.write(config.timestamp + '\\n')\n",
        "            log_handle.close()"
      ],
      "metadata": {
        "id": "TQl3FlXbtkcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class args():\n",
        "    def __init__(self, data , device):\n",
        "        self.data = data\n",
        "        self.device = device\n",
        "\n",
        "dictm = args('data.pt' , 'coda')\n",
        "print (dictm.data)\n",
        "run (dictm,train_init)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "m7KvsAdSt4HD",
        "outputId": "84a6a2f7-3d15-4b9c-bf14-b610b47b34f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3ae49118ad58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdictm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.pt'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'coda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdictm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrun\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdictm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-66c05fa3554a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args, init_task)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# ---- initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'init' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geomloss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOogbL3LRA8G",
        "outputId": "f9fb12e3-e5d9-41cf-f9ee-468217f94074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting geomloss\n",
            "  Downloading geomloss-0.2.5.tar.gz (26 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from geomloss) (1.21.6)\n",
            "Building wheels for collected packages: geomloss\n",
            "  Building wheel for geomloss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for geomloss: filename=geomloss-0.2.5-py3-none-any.whl size=32069 sha256=d27d66ada3cc558b300620cc1a6c20609e6c2492fef7a7fd8d43902199727ace\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/ba/ac/e61f794175073650207d2ad91bea30e2553fc83f74fc9a6b82\n",
            "Successfully built geomloss\n",
            "Installing collected packages: geomloss\n",
            "Successfully installed geomloss-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "import tqdm\n",
        "\n",
        "from collections import OrderedDict\n",
        "from types import SimpleNamespace\n",
        "from time import strftime, localtime\n",
        "\n",
        "import argparse\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import sklearn.decomposition\n",
        "\n",
        "#from .util import *\n",
        "\n",
        "# ---- PRESCIENT\n",
        "\n",
        "class IntReLU(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(IntReLU, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.max(torch.zeros_like(x), 0.5 * (x**2)) # + self.c)\n",
        "\n",
        "\n",
        "class AutoGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AutoGenerator, self).__init__()\n",
        "\n",
        "        self.x_dim = 30\n",
        "        self.k_dim = 500\n",
        "        self.layers = 1\n",
        "\n",
        "        self.activation = 'softplus'\n",
        "        if self.activation == 'relu':\n",
        "            self.act = nn.LeakyReLU\n",
        "        elif self.activation == 'softplus':\n",
        "            self.act = nn.Softplus\n",
        "        elif self.activation == 'intrelu': # broken, wip\n",
        "            raise NotImplementedError\n",
        "        elif self.activation == 'none':\n",
        "            self.act = None\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.net_ = []\n",
        "        for i in range(self.layers):\n",
        "            # add linear layer\n",
        "            if i == 0:\n",
        "                self.net_.append(('linear{}'.format(i+1), nn.Linear(self.x_dim, self.k_dim)))\n",
        "            else:\n",
        "                self.net_.append(('linear{}'.format(i+1), nn.Linear(self.k_dim, self.k_dim)))\n",
        "            # add activation\n",
        "            if self.activation == 'intrelu':\n",
        "                raise NotImplementedError\n",
        "            elif self.activation == 'none':\n",
        "                pass\n",
        "            else:\n",
        "                self.net_.append(('{}{}'.format(self.activation, i+1), self.act()))\n",
        "        self.net_.append(('linear', nn.Linear(self.k_dim, 1, bias = False)))\n",
        "        self.net_ = OrderedDict(self.net_)\n",
        "        self.net = nn.Sequential(self.net_)\n",
        "\n",
        "        net_params = list(self.net.parameters())\n",
        "        net_params[-1].data = torch.zeros(net_params[-1].data.shape) # initialize\n",
        "\n",
        "    def _step(self, x, dt, z):\n",
        "        sqrtdt = np.sqrt(dt)\n",
        "        return x + self._drift(x) * dt + z * sqrtdt\n",
        "\n",
        "    def _pot(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def _drift(self, x):\n",
        "        x_ = x.requires_grad_()\n",
        "        pot = self._pot(x_)\n",
        "\n",
        "        drift = torch.autograd.grad(pot, x_, torch.ones_like(pot),\n",
        "            create_graph = True)[0]\n",
        "        return drift\n",
        "\n",
        "# ---- loss\n",
        "\n",
        "class OTLoss():\n",
        "\n",
        "    def __init__(self, device):\n",
        "\n",
        "        self.ot_solver = SamplesLoss(\"sinkhorn\", p = 2, blur = 0.1,\n",
        "            scaling = 0.7, debias = True)\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, a_i, x_i, b_j, y_j, requires_grad = True):\n",
        "\n",
        "        a_i = a_i.to(self.device)\n",
        "        x_i = x_i.to(self.device)\n",
        "        b_j = b_j.to(self.device)\n",
        "        y_j = y_j.to(self.device)\n",
        "\n",
        "        if requires_grad:\n",
        "            a_i.requires_grad_()\n",
        "            x_i.requires_grad_()\n",
        "            b_j.requires_grad_()\n",
        "\n",
        "        loss_xy = self.ot_solver(a_i, x_i, b_j, y_j)\n",
        "        return loss_xy"
      ],
      "metadata": {
        "id": "Fl7t6pnvuE_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#util\n",
        "\n",
        "# shared functions and classes, including the model and `run`\n",
        "# which implements the main pre-training and training loop\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "from types import SimpleNamespace\n",
        "from time import strftime, localtime\n",
        "\n",
        "import argparse\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import sklearn.decomposition\n",
        "\n",
        "# ---- convenience functions\n",
        "\n",
        "def p_samp(p, num_samp, w = None):\n",
        "    repflag = p.shape[0] < num_samp\n",
        "    p_sub = np.random.choice(p.shape[0], size = num_samp, replace = repflag)\n",
        "    if w is None:\n",
        "        w_ = torch.ones(len(p_sub))\n",
        "    else:\n",
        "        w_ = w[p_sub].clone()\n",
        "    w_ = w_ / w_.sum()\n",
        "\n",
        "    return p[p_sub,:].clone(), w_\n",
        "\n",
        "def fit_regularizer(samples, pp, burnin, dt, sd, model, device):\n",
        "\n",
        "    factor = samples.shape[0] / pp.shape[0]\n",
        "\n",
        "    z = torch.randn(burnin, pp.shape[0], pp.shape[1]) * sd\n",
        "    z = z.to(device)\n",
        "\n",
        "    for i in range(burnin):\n",
        "        pp = model._step(pp, dt, z = z[i,:,:])\n",
        "\n",
        "    pos_fv = -1 * model._pot(samples).sum()\n",
        "    neg_fv = factor * model._pot(pp.detach()).sum()\n",
        "\n",
        "    return pp, pos_fv, neg_fv\n",
        "\n",
        "def pca_transform(x):\n",
        "\n",
        "    pca = sklearn.decomposition.PCA(n_components = 50)\n",
        "    # keep track of how to break up the array after concat\n",
        "    x_ = torch.cat(x)\n",
        "    x_ = pca.fit_transform(x_)\n",
        "    x_breaks = np.append([0], np.cumsum([len(x_) for x_ in x]))\n",
        "    x_tmp = []\n",
        "    for i in range(len(x_breaks) - 1):\n",
        "        ii = x_breaks[i]\n",
        "        jj = x_breaks[i+1]\n",
        "        x_tmp.append(torch.from_numpy(x_[ii:jj]).float())\n",
        "    x = x_tmp\n",
        "\n",
        "    return x\n",
        "\n",
        "def get_weight(w, time_elapsed):\n",
        "    return w\n",
        "\n",
        "def init(args):\n",
        "\n",
        "#    args.cuda = torch.cuda.is_available()\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device('cuda')\n",
        "      kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "    else:\n",
        "      device = torch.device('cpu')\n",
        "      kwargs = {}\n",
        "\n",
        "    return device, kwargs\n",
        "\n",
        "def weighted_samp(p, num_samp, w):\n",
        "    ix = list(torch.utils.data.WeightedRandomSampler(w, num_samp))\n",
        "    return p[ix,:].clone()"
      ],
      "metadata": {
        "id": "6I5oxb-GRFzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tqdm\n",
        "from time import strftime, localtime\n",
        "\n",
        "#from .model import *\n",
        "#from .util import *\n",
        "\n",
        "def run( init_task):\n",
        "\n",
        "    # ---- initialize\n",
        "\n",
        "    device, kwargs = init()\n",
        "    torch.manual_seed(2)\n",
        "    np.random.seed(2)\n",
        "\n",
        "    x, y, w, config = init_task()\n",
        "\n",
        "    # ---- model\n",
        "\n",
        "    model = AutoGenerator(config)\n",
        "    print(model)\n",
        "    model.zero_grad()\n",
        "\n",
        "    # ---- loss\n",
        "\n",
        "    #if args.loss == 'euclidean':\n",
        "    loss = OTLoss(config, device)\n",
        "  #  else:\n",
        "   #     raise NotImplementedError\n",
        "\n",
        "    torch.save(config.__dict__, config.config_pt)\n",
        "\n",
        "    if args.pretrain:\n",
        "\n",
        "        if os.path.exists(config.done_log):\n",
        "\n",
        "            print(config.done_log, ' exists. Skipping.')\n",
        "\n",
        "        else:\n",
        "\n",
        "            model.to(device)\n",
        "            x_last = x[config.train_t[-1]].to(device) # use the last available training point\n",
        "            optimizer = optim.SGD(list(model.parameters()), lr = 1e-9)\n",
        "\n",
        "            pbar = tqdm.tqdm(range(500))\n",
        "            for epoch in pbar:\n",
        "\n",
        "                pp, _ = p_samp(x_last,2000)\n",
        "#  config.t = y[-1] - y[0]\n",
        "                dt = config.t / 50\n",
        "                pp, pos_fv, neg_fv = fit_regularizer(x_last, pp,\n",
        "                    config.pretrain_burnin, dt, config.pretrain_sd,\n",
        "                    model, device)\n",
        "                fv_tot = pos_fv + neg_fv\n",
        "\n",
        "                fv_tot.backward()\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "\n",
        "                pbar.set_description('[{}|pretrain] {} {:.3f}'.format(\n",
        "                    config.out_name, epoch, fv_tot.item()))\n",
        "\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "            }, config.pretrain_pt)\n",
        "\n",
        "    if args.train:\n",
        "\n",
        "        if os.path.exists(config.done_log):\n",
        "\n",
        "            print(config.done_log, ' exists. Skipping.')\n",
        "\n",
        "        else:\n",
        "\n",
        "            checkpoint = torch.load(config.pretrain_pt)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model.to(device)\n",
        "\n",
        "            optimizer = optim.Adam(list(model.parameters()), lr = config.train_lr)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.9)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pbar = tqdm.tqdm(range(config.train_epochs))\n",
        "            x_last = x[config.train_t[-1]].to(device) # use the last available training point\n",
        "            # fit on time points\n",
        "\n",
        "            best_train_loss_xy = np.inf\n",
        "            log_handle = open(config.train_log, 'w')\n",
        "\n",
        "            for epoch in pbar:\n",
        "\n",
        "                losses_xy = []\n",
        "                config.train_epoch = epoch\n",
        "\n",
        "                for j in config.train_t:\n",
        "\n",
        "                    t_cur = j\n",
        "                    t_prev = config.start_t\n",
        "                    dat_cur = x[t_cur]\n",
        "                    dat_prev = x[t_prev]\n",
        "                    y_cur = y[t_cur]\n",
        "                    y_prev = y[t_prev]\n",
        "                    time_elapsed = y_cur - y_prev\n",
        "\n",
        "                    w_prev = get_weight(w[(y_prev, y_cur)], time_elapsed)\n",
        "\n",
        "                    x_i, a_i = p_samp(dat_prev, int(dat_prev.shape[0] * args.train_batch),\n",
        "                        w_prev)\n",
        "                    x_i = x_i.to(device)\n",
        "                    num_steps = int(np.round(time_elapsed / config.train_dt))\n",
        "                    for _ in range(num_steps):\n",
        "                        z = torch.randn(x_i.shape[0], x_i.shape[1]) * config.train_sd\n",
        "                        z = z.to(device)\n",
        "                        x_i = model._step(x_i, dt = config.train_dt, z = z)\n",
        "\n",
        "                    y_j, b_j = p_samp(dat_cur, int(dat_cur.shape[0] * args.train_batch))\n",
        "\n",
        "                    loss_xy = loss(a_i, x_i, b_j, y_j)\n",
        "                    losses_xy.append(loss_xy.item())\n",
        "\n",
        "                    #[F_i, G_j, dx_i] = torch.autograd.grad( Loss_xy, [a_i, b_j, x_i] )\n",
        "\n",
        "                    loss_xy.backward()\n",
        "\n",
        "                train_loss_xy = np.mean(losses_xy)\n",
        "\n",
        "                # fit regularizer\n",
        "\n",
        "                if config.train_tau > 0:\n",
        "\n",
        "                    pp, _ = p_samp(x_last, config.ns)\n",
        "\n",
        "                    dt = config.t / config.train_burnin\n",
        "                    pp, pos_fv, neg_fv = fit_regularizer(x_last, pp,\n",
        "                        config.train_burnin, dt, config.train_sd,\n",
        "                        model, device)\n",
        "                    fv_tot = pos_fv + neg_fv\n",
        "                    fv_tot *= config.train_tau\n",
        "                    fv_tot.backward()\n",
        "\n",
        "                # step\n",
        "\n",
        "                if config.train_clip > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.train_clip)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "\n",
        "                # report\n",
        "\n",
        "                desc = \"[{}|train] {}\".format(config.out_name, epoch + 1)\n",
        "                if len(losses_xy) < 10:\n",
        "                    for l_xy in losses_xy:\n",
        "                        desc += \" {:.6f}\".format(l_xy)\n",
        "                desc += \" {:.6f}\".format(train_loss_xy)\n",
        "                desc += \" {:.6f}\".format(best_train_loss_xy)\n",
        "                pbar.set_description(desc)\n",
        "                log_handle.write(desc + '\\n')\n",
        "                log_handle.flush()\n",
        "\n",
        "                if train_loss_xy < best_train_loss_xy:\n",
        "                    best_train_loss_xy = train_loss_xy\n",
        "\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'epoch': config.train_epoch + 1,\n",
        "                    }, config.train_pt.format('best'))\n",
        "\n",
        "                # save model every x epochs\n",
        "\n",
        "                if (config.train_epoch + 1) % config.save == 0:\n",
        "                    epoch_ = str(config.train_epoch + 1).rjust(6, '0')\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'epoch': config.train_epoch + 1,\n",
        "                    }, config.train_pt.format('epoch_{}'.format(epoch_)))\n",
        "\n",
        "            log_handle.close()\n",
        "\n",
        "            log_handle = open(config.done_log, 'w')\n",
        "            timestamp = strftime(\"%a, %d %b %Y %H:%M:%S\", localtime())\n",
        "            log_handle.write(config.timestamp + '\\n')\n",
        "            log_handle.close()"
      ],
      "metadata": {
        "id": "b_lHHPKrReff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "########### arg\n",
        "Step 1 => editing process data\n"
      ],
      "metadata": {
        "id": "xOtMCROGOVOE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xr-5XYWoQ5IH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}